{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084c90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1756c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM:\n",
    "    def __init__(self, input_size, output_size, recurrences, learning_rate):\n",
    "        self.x = np.zeros(input_size + output_size)\n",
    "        self.input = input_size + output_size\n",
    "        self.y = np.zeros(output_size)\n",
    "        self.output = output_size\n",
    "        self.cs = np.zeros(output_size)\n",
    "        self.recurrences = recurrences\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.f = np.random.randn(output_size, self.input) * 0.1\n",
    "        self.i = np.random.randn(output_size, self.input) * 0.1\n",
    "        self.c = np.random.randn(output_size, self.input) * 0.1\n",
    "        self.o = np.random.randn(output_size, self.input) * 0.1\n",
    "        \n",
    "        # RMSprop memory\n",
    "        self.Gf, self.Gi, self.Gc, self.Go = [np.zeros_like(m) for m in [self.f, self.i, self.c, self.o]]\n",
    "\n",
    "    def sigmoid(self, x): return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    def tangent(self, x): return np.tanh(x)\n",
    "\n",
    "    def forwardProp(self, x_input):\n",
    "        self.x = x_input\n",
    "        f = self.sigmoid(np.dot(self.f, self.x))\n",
    "        self.cs *= f\n",
    "        i = self.sigmoid(np.dot(self.i, self.x))\n",
    "        c_cand = self.tangent(np.dot(self.c, self.x))\n",
    "        self.cs += i * c_cand\n",
    "        o = self.sigmoid(np.dot(self.o, self.x))\n",
    "        self.y = o * self.tangent(self.cs)\n",
    "        return self.cs, self.y, f, i, c_cand, o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25de1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. DATA LOADING & PREPROCESSING ---\n",
    "def load_and_process_data(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as f: f.write(\"hello world. this is a test for the lstm network. hello again.\")\n",
    "    \n",
    "    data = open(file_path, 'r').read().lower()\n",
    "    chars = sorted(list(set(data)))\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "    \n",
    "    return data, char_to_ix, ix_to_char, vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f2fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TRAINING & TESTING (SAMPLING) LOOP ---\n",
    "def run_model():\n",
    "    # Setup\n",
    "    data, char_to_ix, ix_to_char, vocab_size = load_and_process_data('input.txt')\n",
    "    hidden_size = 100 \n",
    "    seq_length = 25 \n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    # Initialize LSTM\n",
    "    # input_size to class = vocab_size (one-hot) + hidden_size\n",
    "    model = LSTM(vocab_size, hidden_size, seq_length, learning_rate)\n",
    "    \n",
    "    # Simple Sampling/Testing Function\n",
    "    def sample(h_prev, c_prev, seed_ix, n):\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "        h, c = h_prev, c_prev\n",
    "        for t in range(n):\n",
    "            # Concatenate h and x for LSTM input\n",
    "            combined_input = np.vstack((h.reshape(-1,1), x)).flatten()\n",
    "            c, h, f, i, cc, o = model.forwardProp(combined_input)\n",
    "            \n",
    "            # Simple projection to vocab size (simplified for this standalone version)\n",
    "            # In a full RNN, you'd have an extra Weight matrix here.\n",
    "            # For this test, we just take the first 'vocab_size' bits of hidden state.\n",
    "            logits = h[:vocab_size] \n",
    "            p = np.exp(logits) / np.sum(np.exp(logits))\n",
    "            ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "            \n",
    "            x = np.zeros((vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        return ''.join(ix_to_char[i] for i in ixes)\n",
    "\n",
    "    # Testing the untrained model\n",
    "    h_init = np.zeros(hidden_size)\n",
    "    c_init = np.zeros(hidden_size)\n",
    "    print(\"--- Initial Test (Untrained) ---\")\n",
    "    print(sample(h_init, c_init, char_to_ix[data[0]], 30))\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7848751",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
